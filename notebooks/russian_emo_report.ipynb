{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e3ee484-68ad-434c-8fb3-28c42fc8e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio, HTML\n",
    "from IPython.core.display import display\n",
    "\n",
    "SAMPLE_RATE = 96000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3990e709-ee26-414d-b522-8b6ae397cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"russian_emo\")\n",
    "WAVS_DIRS = [\"chunk2\", \"chunk3\", \"chunk32\", \"chunk41\", \"chunk111\"]\n",
    "FILE_NAMES = [\"reference\", \"sad\", \"very_happy\", \"happy\", \"very_angry\", \"angry\"]\n",
    "TO_IGNORE_LIST = [DATA_PATH / \".ipynb_checkpoints\", DATA_PATH / \"inference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a7f4b9-6e2c-4c46-b6d5-975a1ac27297",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for d in DATA_PATH.iterdir():\n",
    "    if d.is_dir() and d not in TO_IGNORE_LIST:\n",
    "        audios.append([])\n",
    "        for f in FILE_NAMES:\n",
    "            audio, sr = torchaudio.load(f\"{d}/{f}.wav\")\n",
    "            audios[-1].append(Audio(data=audio, rate=sr)._repr_html_().strip())\n",
    "df = pd.DataFrame(np.array(audios))\n",
    "df.columns = FILE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de867933-a55f-4689-bb1c-17db6cf0fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audios with neutral reference(for comparison) and generated with emotions\n"
     ]
    }
   ],
   "source": [
    "print(\"Audios with neutral reference(for comparison) and generated with emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91644706-b6e2-44b9-b019-5c11aabdc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(ipd.HTML(df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87756c09-7a87-46a2-9dce-bba4f5c79f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated from text with emotions\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated from text with emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddceeaab-a286-4bb4-8cdc-ade0fbd4c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for d in (DATA_PATH / \"inference\").iterdir():\n",
    "    if d.is_dir() and d not in [DATA_PATH / \".ipynb_checkpoints\"]:\n",
    "        audios.append([])\n",
    "        for f in FILE_NAMES[1:]:\n",
    "            audio, sr = torchaudio.load(f\"{d}/{f}.wav\")\n",
    "            audios[-1].append(Audio(data=audio, rate=sr)._repr_html_().strip())\n",
    "df = pd.DataFrame(np.array(audios))\n",
    "df.columns = FILE_NAMES[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "377667c3-af9b-414d-ae17-04b768b3bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(ipd.HTML(df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d7527-441e-4099-860d-1c78401413a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
