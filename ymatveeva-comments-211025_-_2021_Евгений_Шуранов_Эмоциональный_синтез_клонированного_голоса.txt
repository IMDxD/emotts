
слайд  2 "Критерии оценки":
   -- на мой взгляд критерии из шаблона больше заточены на алгоритмические задачи, чем на задачи машинного обучения (когда результат один, фиксированный и детерминированный, и сложность лишь в том, чтобы его побыстрее и поэффективнее достичь).
   Я бы добавила в пункт 1: качество работы финальной модели, оцениваемое через стандартные метрики для выбранной области (для синтеза речи: MOS на естественность речи и отсутствие в ней артефактов, точность контроля за прозодией речи,   similarity на похожесть синтезируемого голоса на оригинал по тембру (везде выше 2.5)). В пункт 3: за дополнительное усложнение поставленной задачи (клонирование вместо синтеза), повышенную сложность используемых технологий (обучение энкодера референс аудио), достижение особо высоких метрик оценки финальной модели (MOS на естественность речи, точность контроля за прозодией речи).

слайд 4: ну как-то чересчур утрированно. 
 Никто не боится чат-ботов, но чем больше они похожи на человека, тем приятнее человеку с ними взаимодействовать.
 А чем приятнее ему с ними взаимодействовать, тем больше будет __частота__ сессий этого взаимодействия. Одну сессию продлевать практически невозможно: у нас есть конкретная цель взаимоействия с ассистентом на сессию, мы ее достигаем и уходим заниматься своими делами. Продление сессии - не вариант в большинстве случаев. 
 Далее советую просто сослаться на исследование Амазона (ссылка в моем слайде) о том, что эмоциональная речь стандартно воспринимается человеком как более естественная (and human-like) чем "безэмоциональный речетатив". Эмоциональность это таким образом одно из определяющих свойств человеческой речи.
 
 Слайды 4 и 5: узковатый взгляд. Чат-ботами все не ограничивается. Смотрите мой слайд, присылаю в чат.
 
 Слайд 5: неверный, хотя бы исходя из ссылок на демки, которые должны были остаться в моем обзоре статей по ЭМО, который мы вым высылали по почте. Но там про академические решения. В плане бизнес-решений можете погуглить IBM, Microsoft, Amazon Polly. На TTS API. Каждый предлагает модификацию интонаций либо по эмоциям, либо по стилю речи в более абстрактном понимании. Но последнее пока что превалирует. Однако технологии для первого и второго (как видно из обзора статей) используются аналогичные. Проблема в том, что по стилям речи данные проще собирать, чем по узко-определенным эмоциям.
 
 Слайд 6: не думаю, что данные с академической лицензией корректно называть закрытыми. Закрытые это все же те, что Амазон для себя собирает и после этого никому и никогда не отдает. Таковым является, например, наш русский датасет хуавейный. Но никак не mdp-podcast / msp-improv.
 
 Слайд 7:
 
 Про оригинальность проекта мы уже комментировали. Уточняю: 1) отсутствие общедоступных моделей, обученных на открытых данных и при этом достигающих уровень коммерческого качества синтеза. 2) отустствие общедоступных моделей, совмещающих в себе приемлемое качество синтеза и точности передавания эмоций с минимумом потребности в обучающих (эмоциональных) данных. НО: здесь надо помнить, что ваш бэкап-план это как раз обучение на нашем закрытом датасете (план Б). В этом случае ваше решение будет воспроизведением уже существующих решений (смотри демки из статей). 
 
 Про решение существующих недоработок не понимаю, о чем пункт вообще.
 
 Интеграция в существующие сервисы: надо ли добавить нерусскоязычны решения (английский) ? 
 
 Слайд 9: HifiGAN: не имплементация модели, а ее дообучение. Имплементировать же с нуля никто не собирается ? Код выложен авторами и находится в общем доступе. 
 
 Последний комментарий: на целый проект заниматься только вокодером, как мне кажется, работы не хватит. Там нужно разобраться в основном механиме тюнинга, после этого работа будет однообразная по запуску скрипта этого тюнинга. При этом весь код там уже есть в доступе. Потому на вторую половину проекта, думаю, нужно с Аней поделиться работой из других частей проекта. Не жадничайте ;)))
 Там есть отдельный относительно независимый и несущий отдельную немалую ценность кусок по исследованию vocoder capacity на разного рода "tricky audios" *что мы обсуждали в прошлый раз и что теперь, как я понимаю, называется vocoder RnD), но и его до конца проекта, кажется, будет мало. Ну впрочем смотрите сами.
 
 
 
 
 
 
 

